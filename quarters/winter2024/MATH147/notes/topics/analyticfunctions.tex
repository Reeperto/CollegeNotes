\documentclass[../notes.tex]{subfiles}
\graphicspath{
    {'../figures'}
}

\begin{document}

\banner{Analytic Functions}

\subsection{Complex Functions}

\begin{definition}[Complex Function]
    A complex function on $S \subset \C$ is a rule that assigns to each $z \in S$ a value $f(z) = w \in \C$, denoted by $f : S \to \C$.
\end{definition}

\begin{example}
    There are (surprise!) many complex functions.
    \begin{enumerate}
        \item The function $f(z) = \frac{1}{z}$ is well defined everywhere except $z = 0$, therefore it's domain of definition is $\C \setminus \qty{0}$.
        \item Any complex polynomial $f(z) = c_n z^n + \ldots + c_1 z + c_0$ with $c_i \in \C$ is a complex function over all of $\C$.
        \item Any rational function $\frac{f(x)}{g(x)}$ where the domain is $\C \setminus \qty{z \in \C : g(z) = 0}$
    \end{enumerate}
\end{example}

A complex function can also often be represented in the form
\[
    f(x+iy) = u(x,y) + i v(x, y)
.\]
Consider the case of $\frac{1}{z}$. Then
\[
    \frac{1}{x+iy} = \frac{x-iy}{x^2+y^2} = \frac{x}{x^2+y^2} - i \cdot \frac{y}{x^2+y^2}
.\]
Therefore in this case $u(x,y) = \frac{x}{x^2+y^2}$ and $v(x,y) = \frac{y}{x^2+y^2}$.

\begin{definition}[Limits in $\C$]
    The limit of a function $f:\dom f \to \C$
    \[
        \lim_{z \to z_0} f(z) = w_0
    \]
    means that 
    \[
        \forall \epsilon > 0, \exists \delta > 0, \st |z-z_0| < \delta \implies |f(z) - w_0| < \epsilon, \forall z
    .\]
    That is, for any $\epsilon$ neighborhood of $w_0$, there is some deleted $\delta$ neighborhood around $z_0$ such that every $z$ in the $\delta$ neighborhood maps into the $\epsilon$ neighborhood.
\end{definition}

\begin{example}
    Consider the function $f(z) = \frac{i}{2} \conj{z}$. One can guess that
    \[
        \lim_{z \to 1} f(z) = \frac{i}{2} 1 = \frac{i}{2}
    .\]
    For this to happen,
    \begin{align*}
        \qty|\frac{i}{2} \conj{z} - \frac{i}{2}| < \epsilon \implies \qty|\frac{i}{2}| |\conj{z} - 1| &< \epsilon \\
        \frac{1}{2} |\conj{z} - 1| &< \epsilon \\
        \frac{1}{2} |z - 1| &< \epsilon \\
        |z - 1| &< 2 \epsilon
    \end{align*}
    Therefore choosing $\delta = 2 \epsilon$ gives the desired result.
\end{example}

\begin{example}
    Consider $f(z) = \frac{\conj{z}}{z}$. Does $f(z)$ have a limit at $z_0 = 0$? Note that along the real axis, $z = x$ and $\conj{z} = x$, hence the limit is $\lim_{x\to 0}\frac{x}{x} = 1$. Along the imaginary axis, $z = y$ and $\conj{z} = -y$, meaning the limit is $\lim_{y\to 0} \frac{-y}{y} = -1$. Therefore there is no limit.
\end{example}

\begin{theorem}[Limit Equivalence]
    \label{thm:limitequivalence}
    If $f(z) = u(z) + iv(z)$ where $u$ and $v$ are real valued functions, then
    \[
        \renewcommand{\arraystretch}{1.5}
        \lim_{z \to z_0} f(z) = u_0 + iv_0 \Longleftrightarrow \begin{array}{rl}
            \displaystyle
            \lim_{z\to z_0} u(z) &= u_0 \\
            \displaystyle
            \lim_{z\to z_0} v(z) &= v_0
        \end{array}
    .\]
\end{theorem}

\subsection{Continuity}

\begin{definition}[Continuity]
    A function $f : \dom f \to \C$ is continuous at $z_0 \in \C$ if
    \[
        \lim_{z \to z_0} f(z) = f(z_0)
    .\]
    That is, the limit exists, $f(z_0)$ exists, and that they are equal. The epsilon-delta form is
    \[
        \forall \epsilon > 0, \exists \delta > 0 \st |z - z_0| < \delta \implies |f(z) - f(z_0)| < \epsilon
    .\]
\end{definition}

\begin{example}
    Is $f(z) = \conj{z}$ continuous? That is does $\lim_{z \to z_0} f(z) = \conj{z_0}$? Fix $\epsilon > 0$ and take $\delta = \epsilon$. Note then that
    \[
        |z - z_0| < \delta \implies |\conj{z - z_0}| < \epsilon \implies |\conj{z} - \conj{z_0}| < \epsilon
    .\]
    Therefore $f(z)$ is continuous for all $z \in \C$.
\end{example}

\begin{example}
    Consider $f(z) = \Arg z$. Intuitively, it is not continuous since it is always possible to find two points on opposites side the real axis that get arbitrarily close but will have a difference of $2 \pi$.
\end{example}

\begin{theorem}[Continuity Results]
    \label{thm:continuityresults}
    Let $f, g$ be continuous functions at $z_0$. Then
    \begin{enumerate}
        \item $f+g$ is continuous at $z_0$
        \item $f\cdot g$ is continuous at $z_0$
        \item $\frac{f}{g}$ is continuous at $z_0$ if $g(z_0) \neq 0$
        \item If $g$ is continuous at $f(z_0)$, then $g\circ f$ is continuous at $z_0$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    If $f(z)$ is continuous at $z_0$ and $f(z_0) \neq 0$, then there is some neighborhood of $z_0$ where $f(z) \neq 0$.
\end{theorem}

\begin{proof}
    Let $\epsilon = \frac{|f(z_0)|}{2}$. Since $f$ is continuous at $z_0$, there is some $\delta > 0$ such that $|z - z_0| < \delta \implies |f(z) - f(z_0)| < \epsilon$. Assume towards contradiction that $f(z) = 0$ for some $z$ where $|z - z_0| < \delta$. Then
    \[
        |f(z) - f(z_0)| = |f(z_0)| < \epsilon = \frac{|f(z_0)|}{2} \implies 1 < \frac{1}{2}
    .\]
    This is a contradiction. Therefore $f(z) \neq 0$ when $|z-z_0| < \delta$. 
\end{proof}

\begin{theorem}
    If $f(z) = u(z) + iv(z)$ and $z_0 = x_0 + i y_0$, then $f$ is continuous at $f(z_0)$ iff $u(z)$ and $v(z)$ are continuous at $z_0$.
\end{theorem}

\begin{theorem}
    Suppose $f$ is continuous on a closed and bounded region $\mathcal{D}$. Then there is some $M \geq 0$ such that
    \[
        |f(z)| \leq M, \forall z \in \mathcal{D}
    \]
    and there is some $z \in \mathcal{D}$ such that $|f(z)| = M$.
\end{theorem}
\begin{proof}
    Let $f(z) = u(x,y) + i v(x,y)$ be continuous on a closed and bounded region $\mathcal{D}$. Therefore
    \[
        (x, y) \mapsto \sqrt{u(x,y)^2 + v(x,y)^2}
    \]
    is also continuous from $\mathcal{D} \to \R$. Since this is a real function on a closed and bounded region, then there is some maximum value $M \geq 0$ that it obtains. Since the function is the modulus, then
    \[
        |f(z)| \leq M, \forall z \in \mathcal{D}
    \]
    and there is a $z \in \mathcal{D}$ where $|f(z)| = M$.
\end{proof}

\subsection{Differentiability}

\begin{theorem}[Cauchy Riemann Equations]
    \label{thm:cauchyriemann}
    Let $f(z) = u + iv$. If $f$ is differentiable at $z_0$, then
    \begin{alignat*}{2}
        \pdv{u}{x} &= &&\pdv{v}{y} \\
        \pdv{u}{y} &= -&&\pdv{v}{x}
    \end{alignat*}
    at $z_0$.
\end{theorem}

\begin{example}
    Consider $f(x+iy) = 2x + ixy^2$. Then
    \begin{align*}
        u(x,y) &= 2x \\
        v(x,y) &= xy^2
    \end{align*}
    Therefore
    \begin{alignat*}{2}
        \pdv{u}{x} &= 2, &&\pdv{u}{y} = 0 \\
        \pdv{v}{x} &= y^2, &&\pdv{v}{y} = 2xy
    \end{alignat*}
    From the first Cauchy Riemann equation, $2 = 2xy \implies xy = 1$. From the second, $0 = -y^2 \implies y = 0$. Notice then that $xy = 0$ for all $x$. Hence the equations are never satisfied and $f$ is differentiable nowhere.
\end{example}

\begin{example}
    Consider $f(z) = e^{\conj{z}}$. Let $z = x + iy$. Then
    \[
        e^{\conj{z}} = e^{x - iy} = e^x e^{-iy} = e^x (\cos (-y) + i \sin (-y)) = e^x (\cos y - i \sin y)
    \]
    Therefore
    \begin{align*}
        u(x,y) &= e^x \cos y \\
        v(x,y) &= -e^x \sin y
    \end{align*}
    The partials are
    \begin{alignat*}{2}
        \pdv{u}{x} &= e^x \cos y, &&\pdv{u}{y} = -e^x\sin y \\
        \pdv{v}{x} &= -e^x \sin y, &&\pdv{v}{y} = -e^x \cos y
    \end{alignat*}
    Checking the first Cauchy Riemann equation gives
    \[
        e^x \cos y = -e^x \cos y \implies 2e^x \cos y = 0 \implies \cos y = 0
    .\]
    Therefore $y = \frac{\pi}{2} + k \pi$ for $k \in \Z$. Checking the second equation gives
    \[
        -e^x \sin y = e^x \sin y \implies 2e^x \sin y = 0 \implies \sin y = 0
    .\]
    This is only true when $y = k \pi$ for $k \in \Z$. However there is no $y$ that satisifies both conditions so $f$ is differentiable nowhere.
\end{example}

\subsubsection{Polar Cauchy Riemann Equations}

\begin{proof}
    Let $f(x + iy) = u(x,y) + iv(x,y)$ and $z_0 \in \C \neq 0$. Subsitute $x = r \cos \theta$ and $y = r \sin \theta$. Thus $u$ and $v$ can be considered functions of $r$ and $\theta$. Using the multivariable chain rule gives
    % TODO: Fix this
    \begin{align*}
        \pdv{u}{r} &= \pdv{u}{x} \pdv{x}{r} + \pdv{u}{y} \pdv{y}{r} \\
                   &= \pdv{u}{x} \cos \theta + \pdv{u}{y} \sin \theta \\ 
                   \\
        \pdv{u}{\theta} &= \pdv{u}{x} \pdv{x}{\theta} + \pdv{u}{y} \pdv{y}{\theta} \\
                   &= \pdv{u}{x} (-r\sin \theta)  + \pdv{u}{y} (r \cos \theta)
    \end{align*}
    Suppose that the Cauchy Riemann equations are satisfied for $f$. Then
    \begin{alignat*}{2}
        \pdv{u}{x} &= &&\pdv{v}{y} \\
        \pdv{u}{y} &= -&&\pdv{v}{x}
    \end{alignat*}
    Therefore
    \begin{alignat*}{2}
        \pdv{v}{r} &= -\pdv{u}{y} \cos \theta + \pdv{u}{x} \sin \theta &&= r \pdv{u}{r} \\
        \pdv{v}{\theta} &= \pdv{u}{y} r\sin \theta + \pdv{u}{x} r\cos \theta &&= -\frac{1}{r} \pdv{u}{\theta}
    \end{alignat*}
    Therefore the following are equivalent to the Cauchy Riemann equations
    \begin{alignat*}{2}
        \pdv{v}{r} &= \;\;\; r&&\pdv{u}{y} \\
        \pdv{v}{\theta} &= -\frac{1}{r}&&\pdv{u}{\theta}
    \end{alignat*}
\end{proof}

\subsubsection{Converse of Cauchy Riemann}

\begin{theorem}[Converse of C.R.]
    If $f = u + iv$ is defined in an $\epsilon$-neighborhood of some $z_0 = x_0 + iy_0$ and
    \begin{enumerate}
        \item The Cauchy Riemann equations hold at $z_0$
        \item $u_x, u_y, v_x, v_y$ exist in the $\epsilon$-neighborhood and are continuous at $z_0$
    \end{enumerate}
    then $f$ is differentiable at $z_0$ and $f'(z_0) = u_x(z_0) + iv_x(z_0)$.
\end{theorem}

% \begin{proof}
%     Note that the change is $u$ is
%     \begin{align*}
%         \Delta u &= u(x_0 + \Delta x, y_0 + \Delta y_0) - u(x_0, y_0) \\
%                  &= \qty[u(x_0 + \Delta x, y_0 + \Delta y_0) - u(x_0, y_0 + \Delta y)] + \qty[u(x_0, y_0 + \Delta y) - u(x_0, y_0)] \\
%     \end{align*}
% \end{proof}

\subsubsection{} %XXX: z bar differential

\subsection{Uniqueness Theorem}

\begin{theorem}[Unqiueness Theorem]
    Suppose $f$ is defined in a domain $\mathcal{D}$ and
    \begin{enumerate}
        \item $f$ is analytic in $\mathcal{D}$
        \item $f(z) = 0$ for all $z$ in some $\ball{z_0, \delta} \subset \mathcal{D}$ or a line segment $L \subset\mathcal{D}$
    \end{enumerate}
    Then $f(z) = 0$ for all $z \in \mathcal{D}$.
\end{theorem}
\begin{proof}[Open Neighborhood]
    % TODO: Make a picture?
    Let $p \in \mathcal{D}$. Since $\mathcal{D}$ is connected, there is a piecewise linear curve $\gamma$ connecting $z_0$ and $p$. Let $ d = \min\qty{\delta, \text{distance from }\gamma\text{ to }\partial\mathcal{D}}$. Construct a finite sequence of points $\qty{z_n} \subset \gamma$ that starts at $z_0$ and ends at $p$ such that
    \[
        |z_k - z_{k-1}| < d, k > 1
    .\]
    For each point $z_i$, let $N_i = \ball{z_i, d}$. Since $d \leq \delta$, $N_0 \subset \ball{z_0, \delta}$ and therefore $f$ is zero on $N_0$. Since $|z_1 - z_0| < \delta$, $z_1 \in \ball{z_0, \delta}$ and therefore $f(z_1) = 0$. There is a later result that will finish this proof.

    \begin{theorem}
        If $f$ is analytic in a neighborhood $N_0$ of some $z_0$ and $f \equiv 0$ on a domain $\mathcal{D}$ or line segment $L$ in $N_0$, then $f \equiv 0$ on $N_0$.
    \end{theorem}

    Therefore $f(z)$ is zero on $N_1$. This same process can be applied iteratevly, and since $p$ is in the last constructed neighborhood, $f(p) = 0$.
\end{proof}

\begin{corollary}
    Suppose $f,g$ are analytic functions on some domain $\mathcal{D}$ and $f \equiv g$ in some domain $\mathcal{D}' \subset \mathcal{D}$ or line segment $L \subset \mathcal{D}$. Then $f \equiv g$ on $\mathcal{D}$.
\end{corollary}

\end{document}
