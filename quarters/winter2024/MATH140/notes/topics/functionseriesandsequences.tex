\documentclass[../notes.tex]{subfiles}
\graphicspath{
    {'../figures'}
}

\begin{document}
\banner{Sequences and Series of Functions}

\subsection{Power Series}

\begin{definition}[Power Series]
    A power series is a real valued function $f(x) = \sum a_n x^n$ for some sequence $(a_n)$.
\end{definition}

\begin{theorem}
    For a power series $\sum a_n x^n$, let $\beta = \limsup |a_n|^{\frac{1}{n}}$ and $R = \frac{1}{\beta}$. The power series converges for $|x| < R$ and diverges for $|x| > R$
\end{theorem}
\begin{proof}
    Apply the root test. Then
    \[
        \limsup |c_n|^{\frac{1}{n}} = \limsup |a_n|^{\frac{1}{n}} |x| = \limsup |a_n|^{\frac{1}{n}} |x| = |x| \beta
    .\]
    Note then that $|x| < R = \frac{1}{\beta}$ means that $\limsup |c_n|^{\frac{1}{n}} < 1$ and therefore the series converges. The opposite is true for $|x| > R$.
\end{proof}

\begin{example}
    Consider $\sum x^n$. Note that $a_n = 1$ for all $n \in \N$. Therefore $\limsup |a_n|^{\frac{1}{n}} = \limsup 1^{\frac{1}{n}} = 1$. Therefore the power series converges for all $|x| < 1$. Note that $x = 1$ gives a divergent series and $x = -1$ gives an alternating series whose non alternative part does not go to zero and hence also diverges.
\end{example}

\begin{example}
    Consider $\sum \frac{x^n}{n!}$. In this instance $a_n = \frac{1}{n!}$. Then
    \[
        \beta = \limsup |a_n|^{\frac{1}{n}} = \limsup \qty|\frac{1}{n!}|^{\frac{1}{n}}
    .\]
    This would be hard to compute. However, if this limit exists, then it matches the value of the ratio test and therefore
    \[
        \beta = \limsup \qty|\frac{a_{n+1}}{a_n}| = \limsup \frac{1}{(n+1)!} \cdot \frac{n!}{1} = \limsup \frac{1}{n} = 0
    .\]
    Therefore $R = +\infty$ meaning the interval of convergence is all of $\R$.
    \begin{remark}
        Alternatively, one can use the Sterling approximation of the factorial to do the root test. The Sterling approximation is
        \[
            n! \sim \qty(\frac{n}{e})^n \sqrt{2 \pi n}
        .\]
        Hence
        \[
            \limsup \qty|\frac{1}{n!}|^{\frac{1}{n}} = \limsup \frac{1}{\qty(\qty(\frac{n}{e})^n \sqrt{2 \pi n})^{\frac{1}{n}}} = \limsup \frac{1}{\frac{n}{e} \cdot \qty(\sqrt{2 \pi n})^{\frac{1}{n}}} = \limsup \frac{1}{n} = 0
        .\]
    \end{remark}
\end{example}

\begin{example}
    Consider $\sum \frac{x^n}{n^2}$. Then
    \[
    \beta = \limsup \qty(\frac{1}{n^2})^{\frac{1}{n}} = \limsup \frac{1}{\sqrt[n]{n^2}} = 1
    .\]
    Therefore the power series converges for $|x| < 1$. Importantly, for $x = 1$ and $x = -1$, you get convergent series and therefore the interval of convergence is $[-1,1]$.
\end{example}

\begin{example}
    Consider $\sum \frac{(-1)^{n+1} x^n}{n}$. Then $a_n = \frac{(-1)^{n+1}}{n}$ and
    \[
        \beta = \limsup \qty|\frac{(-1)^{n+1}}{n}|^{\frac{1}{n}} = \limsup \frac{1}{\sqrt[n]{n}} = \frac{1}{1} = 1
    .\]
    Therefore the power series converges for $|x| < 1$. Checking $x = 1$,
    \[
        \sum \frac{(-1)^{n+1}}{n} \text{ converges by alternating series test}
    .\]
    And checking for $x = -1$,
    \[
        \sum \frac{(-1)^{2n+1}}{n} = \sum \frac{-1}{n} = - \sum \frac{1}{n} \text{ which diverges}
    .\]
    Therefore the interval of convergence is $(-1, 1]$.
\end{example}

\begin{example}
    Consider $\sum \frac{(2n)! x^n}{(n!)^2}$. Then $a_n = \frac{(2n)!}{(n!)^2}$. Apply the ratio test to get $\beta$.
    \[
        \beta = \limsup \qty|\frac{a_{n+1}}{a_n}| = \limsup \frac{(2n+2)!}{((n+1)!)^2} \cdot \frac{(2n)!}{(n!)^2} = \limsup \frac{(2n+2)(2n+1)}{(n+1)(n+1)} = 4
    .\]
    Therefore it converges on $|x| < \frac{1}{4}$. Checking the endpoints suck but $x = \frac{1}{4}$ diverges by using Sterlings approximation and $x = -\frac{1}{4}$ converges by the alternating series test by the previous method. Therefore the interval of convergence is $\left[-\frac{1}{4}, \frac{1}{4} \right)$.
\end{example}

\subsection{Uniform Convergence}

An initial, but weak, formulation of functional sequence convergence is by applying the a basic limit of a sequence.

\begin{definition}[Pointwise Convergence]
    A sequence of real value functions $f_n : S \subset \R \to \R$ converges point wise to a function $f$ on $S$ if $\lim_{n\to \infty} f_n(x) = f(x)$ for all $x \in S$
\end{definition}

\begin{definition}[Uniform Convergence]
    A sequence of real value functions $f_n : S \subset \R \to \R$ uniformly converges to a function $f$ on $S$ if $\forall \epsilon > 0$, there is some $N \in \N$ such that
    \[
        |f_n(x) - f(x)| < \epsilon, n > N, \forall x \in S
    .\]
\end{definition}

\begin{example}
    Consider the sequence of functions $f_n(x) = x^n$ on $[0,1]$. Note that for all $n$, $f_n(0) = 0$ and $f_n(1) = 1$. Furthermore, for $0 < x < 1$, $\lim x^n = 0$. Therefore
    \[
        \lim f_n(x) = f(x) = \begin{cases}
            0 & 0 \leq x < 1 \\
            1 & x = 1
        \end{cases}
    .\]
    is the pointwise limit of the sequence. For uniform convergence, we want
    \[
        |f_n(x) - f(x)| < \epsilon \Leftrightarrow \qty|x^n - \begin{cases}
            0 & 0 \leq x < 1 \\
            1 & x = 1
        \end{cases}| < \epsilon
    .\]
    For $x = 1$, the absolute value goes to $0$ and therefore only $0 \leq x < 1$ matters. The question becomes when
    \[
        x^n < \epsilon \implies n > \frac{\ln(\epsilon)}{\ln|x|}
    .\]
    However, it is not possible to bound this quantity since $x \to 1$ leads to $\frac{1}{\ln|x|} \to -\infty$. Therefore the sequence does not uniformly converge to $f$.
\end{example}

\begin{example}
    Let $g_n(x) = (1 - |x|)^n$ on $(-1, 1)$. Note that $\lim g_n(0) = 1$ since $g_n(0) = 1$ for all $n$. For any other $x$, $|x| < 1$ and therefore $1 - |x| < 1$. Hence $\lim g_n(x) = 0$ for $x \neq 0$. Hence
    \[
        \lim g_n(x) = g(x) = \begin{cases}
            1 & x = 0 \\
            0 & x \neq 0
        \end{cases}
    .\]
    Checking for uniform convergence,
    \[
        |g_n(x) - g(x)| < \epsilon \Leftrightarrow \qty| (1-|x|)^n
        \begin{cases}
            1 & x = 0 \\
            0 & x \neq 0
        \end{cases}
        |
    .\]
    We only have to care about $x \neq 0$, therefore
    \[
        |(1-|x|)^n| < \epsilon \implies n > \frac{\ln(\epsilon)}{\ln(1-|x|)}
    .\]
    However, $\sup_{x\in (-1,1)} \frac{\ln(\epsilon)}{\ln(1-|x|)} = +\infty$, therefore the sequence does not uniformly converge to $g(x)$.
\end{example}

\begin{example}
    Let $h_n(x) = \frac{1}{n} \sin(nx)$. Since $\qty|\frac{1}{n} \sin(nx)| \leq \qty|\frac{1}{n}| = \frac{1}{n}$, it follows that
    \[
        0 \leq \lim_{n\to \infty} \qty|\frac{1}{n} \sin(nx)| \leq \lim_{n \to \infty} \frac{1}{n} = 0
    .\]
    Therefore $\lim h_n(x) = 0$. Checking for uniform convergence, we want
    \[
        |h_n(x) - h(x)| < \epsilon \Leftrightarrow \qty|\frac{1}{n} \sin(nx) - 0| < \epsilon
    .\]
    Since $\qty|\frac{1}{n} \sin(nx)| \leq \frac{1}{n}$, choosing $n > \frac{1}{\epsilon}$ gives the desired inequality. Since the bound for $n$ doesnt depend on $x$, the sequence uniformly converges to $h(x) = 0$.
\end{example}

\begin{example}
    Let $j_n(x) = \frac{nx}{2n + 1}$ on $S = [-2, 2]$. It's pointwise limit is
    \[
        \lim j_n(x) = \lim \frac{nx}{2n + 1} = x \lim \frac{n}{2n+1} = \frac{x}{2} = j(x)
    .\]
    Checking for uniform convergence, we want
    \begin{align*}
        \qty|\frac{nx}{2n+1} - \frac{x}{2}| < \epsilon &\implies \qty|\frac{2nx - (2n+1)x}{2(2n+1)}| < \epsilon \\
                                               &\implies \frac{|x|}{2(2n+1)} < \epsilon\\
                                               &\implies \frac{|x|}{2\epsilon} < 2n + 1\\
                                               &\implies n > \frac{|x|}{4\epsilon} - \frac{1}{2}
    \end{align*}
    Since $|x| < 2$, $n > \frac{1}{2\epsilon} - \frac{1}{2} > \frac{|x|}{4\epsilon} - \frac{1}{2}$ gives the original inequality. Therefore the sequence uniformly converges to $j(x)$.
\end{example}

\begin{example}
    Let
    \[
        k_n(x) = \begin{cases}
            1 & x > \frac{1}{n} \\
            0 & x \leq \frac{1}{n}
        \end{cases}
    \]
    on $S = [0,1]$. Note that $0 \leq \frac{1}{n}$ for all $n$, meaning $\lim k_n(0) = 0$. For similar reasoning $1 \geq \frac{1}{n}$ for all $n > 1$ and therefore $\lim k_n(1) = 1$. For any $0 < x <1$, there will be some $N \in \N$ such that $n > N \implies \frac{1}{n} < x$. Hence $\lim k_n(x) = 1$ for all $0 < x < 1$. In total then, the pointwise convergence is
    \[
        k(x) = \begin{cases}
            0 & x = 0 \\
            1 & x \neq 0 \\
        \end{cases}
    .\]
    Checking for uniform convergence, we want
    \[
        |k_n(x) - k(x)| < \epsilon \implies \qty|
        \begin{cases}
            1 & x > \frac{1}{n} \\
            0 & x \leq \frac{1}{n}
        \end{cases} - 
        \begin{cases}
            0 & x = 0 \\
            1 & x \neq 0 \\
        \end{cases}
        | = \qty|
        \begin{cases}
            0 - 0 & x = 0 \\
            0 - 1 & 0 < x \leq \frac{1}{n} \\
            1 - 1 & \frac{1}{n} < x \leq 1
        \end{cases}
        | < \epsilon
    .\]
    Note then that
    \[
        \qty|
        \begin{cases}
            0 - 0 & x = 0 \\
            0 - 1 & 0 < x \leq \frac{1}{n} \\
            1 - 1 & \frac{1}{n} < x \leq 1
        \end{cases}
        | = 
        \begin{cases}
            0 & x = 0, \frac{1}{n} < x \leq 1  \\
            1 & 0 < x \leq \frac{1}{n}
        \end{cases}
    .\]
    Since $0 < x \leq \frac{1}{n}$ the value is $1$, it is not possible to get arbitrarily close to the pointwise convergence across all $x$.
\end{example}

\begin{theorem}
    A sequence of functions $f_n$ uniformly converges to $f$ on $S \subset \R$ iff
    \[
        \lim_{n\to \infty} \sup_{x \in S} \qty{f_n(x) - f(x)}
    .\]
\end{theorem}

\begin{theorem}
    If $f_n \to f$ uniformly on $[a,b]$ and $f_n$ is continuous on $[a,b]$ for all $n$, then
    \[
        \lim_{n\to \infty} \int_a^b f_n(x) dx = \int_a^b f(x) dx
    .\]
\end{theorem}
\begin{proof}
    We want to show that 
    \[
        \forall \epsilon > 0, \exists N, \text{ s.t. } \qty|\int_a^b f_n(x) \dd x - \int_a^b f(x) dx| < \epsilon
    .\]
    Fix $\epsilon > 0$. Then
    \begin{align*}
        \qty|\int_a^b f_n(x) \dd x - \int_a^b f(x) dx| &= \qty|\int_a^b f_n(x) - f(x) \dd x| \\
                               &\leq \int_a^b |f_n(x) - f(x)| \dd x
    \end{align*}
    Since $f_n \to f$ uniformly on $[a,b]$, there is a $N$ such that $|f_n(x) - f(x)| < \frac{\epsilon}{b - a}$ for $n > N$ and $x \in [a,b]$. Note then that
    \[
        \int_a^b |f_n(x) - f(x)| \dd x < \int_a^b \frac{\epsilon}{b - a} \dd x = \epsilon
    .\]
    Therefore for $n > N$,
    \[
        \qty|\int_a^b f_n(x) \dd x - \int_a^b f(x) dx| \leq  \int_a^b |f_n(x) - f(x)| \dd x < \epsilon
    .\]
\end{proof}

\subsection{Cauchy Function Sequences}

\begin{definition}[Uniformly Cauchy]
    A sequence of real valued functions $f_n$ is called unfiormly Cauchy if
    \[
        \forall \epsilon > 0, \exists N \text{ s.t. } |f_n(x) - f_m(x)| < \epsilon, \forall x \in S, n > m > N
    .\]
\end{definition}

\begin{theorem}
    If a sequence of real valued functions $f_n$ is uniformly Cauchy on $S \subset \R$, then there exists some function $f(x)$ on $S$ such that $f_n \to f$ uniformly on $S$.
\end{theorem}

\begin{proof}
    Fix $x \in S$ and let $y_n = f_n(x)$. Note that this gives a Cauchy sequence since $f_n$ is uniformly Cauchy. Therefore $y_n$ converges to some $y \in \R$. Define $F(x) = y$. By construction, $f_n \to F$ pointwise. Fix $\epsilon > 0$. Since $f_n$ is uniformly Cauchy
    \[
        |f_n(x) - f_m(x)| < \frac{\epsilon}{2} \implies -\frac{\epsilon}{2} + f_m(x) < f_n(x) < f_m(x) + \frac{\epsilon}{2}
    .\]
    Since $n > m$, $n$ can be sent to infinity while fixing $m$, giving
    \[
        -\frac{\epsilon}{2} + f_m(x) < F(x) < f_m(x) + \frac{\epsilon}{2} \implies -\frac{\epsilon}{2} < F(x) - f_m(x) < \frac{\epsilon}{2}
    .\]
    Therefore
    \[
        |f_m(x) - F(x)| < \frac{\epsilon}{2} < \epsilon
    .\]
    Therefore $f_m$ converges uniformly to $F$ on $S$.
\end{proof}

\begin{example}
    Consider the series $f_n(x) = \displaystyle\sum_{k=0}^n \frac{1}{1 + x^k}$ on $[2, \infty)$. Trying to determine if this uniformy converges with a direct approach will not work as it requires knowledge about the function the infinite series represents. However, notice that for $n > m$
    \begin{align*}
        |f_n(x) - f_m(x)| &= \qty|\sum_{k=0}^n \frac{1}{1+x^k} - \sum_{j=0}^m \frac{1}{1+x^j}| \\
        &= \qty|\sum_{k=m+1}^n \frac{1}{1+x^k}| \\
        &\leq \qty|\sum_{k=m+1}^n \frac{1}{1+2^k}| \\
        &\leq \qty|\sum_{k=m+1}^n \frac{1}{2^k}|  \\
        &= \cfrac{\cfrac{1}{2^{m+1}} - \cfrac{1}{2^n+1}}{1 - \frac{1}{2}} \\
        &= \frac{1}{2^m} - \frac{1}{2^n} < \frac{1}{2^m}
    \end{align*}
    Take $\epsilon > 0$. Choose $N \in \N$ such that $\frac{1}{2^N} < \epsilon$. Then
    \[
        |f_n(x) - f_m(x)| < \frac{1}{2^m} < \frac{1}{2^N} < \epsilon, n > m > N
    .\]
    Therefore $f_n$ is uniformly Cauchy on $[2, \infty)$. This means that $f_n \to f$ uniformly on $[2, \infty)$.
\end{example}

\begin{example}
    Consider a power series $f(x) = \displaystyle\sum_{k=0}^\infty a_k x^k$ with the sequence of polynomials $f_n(x) = \displaystyle\sum_{k=0}^n a_k x^k$. Let $\beta = \limsup |a_n|^{\frac{1}{n}}$ and $R = \frac{1}{\beta}$. Consider some $0 < \tilde{R} < R$ and $S = (-\tilde{R}, \tilde{R})$. Then
    \begin{align*}
        |f_n(x) - f_m(x)| &= \qty|\sum_{k=m+1}^n a_k x^k| \\
                      &\leq \sum_{k=m+1}^n \qty|a_k x^k| \\
                      &= \sum_{k=m+1}^n \qty(|a_k|^{\frac{1}{k}} |x|)^k \\
                      &\leq \sum_{k=m+1}^n \qty(|a_k|^{\frac{1}{k}} \tilde{R})^k \\
    \end{align*}
    Since $\limsup |a_n|^{\frac{1}{n}} = \beta = \frac{1}{R}$, it is possible to find some $K \in \N$ such that
    \[
        \qty||a_k|^{\frac{1}{k}} - \beta| < \epsilon_1 \text{ with } (\beta + \epsilon_1) \tilde{R} < 1, k > K
    .\]
    Let $\alpha = (\beta + \epsilon_1) \tilde{R} < 1$. Then
    \[
        \sum_{k=m+1}^n \qty(|a_k|^{\frac{1}{k}} \tilde{R})^k < \sum_{k=m+1}^n \alpha^k \leq \frac{a^{m+1}}{1 - \alpha} < \epsilon
    .\]
    with $n > m > K$. This means that $f_n$ is uniformly Cauchy and hence uniformly converges to $f$ in the interval $(-\tilde{R}, \tilde{R})$. This result means that many useful properties about unfirom convergence apply to the interior of the interval of convergence.
\end{example}

\subsection{Differentiation and Integration of Power Series}

\begin{theorem}[Continuity of Power Series]
    If $f(x) = \displaystyle\sum_{n=0}^\infty a_n x^n$ is a power series with radius of convergence $R > 0$, then for any $0 < R' < R$ $f_n = \displaystyle\sum_{k=0}^n a_k x^k$ converges uniformly and $f$ is continuous on $ S = [-R', R']$.
\end{theorem}

\begin{proof}
    Let $f_n(x) = \displaystyle\sum_{k = 0}^n g_k(x)$ with $g_k(x) = a_k x^k$. Applying the $M$ test gives
    \[
        M_k = \sup_{x\in S} |g_k(x)| = |a_k| (R')^k
    .\]
    Appying the root test to the series $\sum |a_k|(R')^k$,
    \[
        \lim_{k\to \infty} |M_k|^{\frac{1}{k}} = \lim_{k \to \infty} |a_k|^{\frac{1}{k}} R' \leq \beta R' = \frac{1}{R} R' < 1
    .\]
    Therefore this series converges and hence $f_n$ converges uniformly on $S$ by the $M$-test. This means that $f$ itself is also continuous on $S$.
\end{proof}

\begin{corollary}
    If $f(x) = \displaystyle \sum_{n=0}^\infty a_n x^n$ has a radius of convergence $R > 0$, then $f$ is continuous on $(-R, R)$.
\end{corollary}

\begin{example}
    Consider $f(x) = \displaystyle \sum_{n=1}^\infty \frac{x^n}{n^2 2^n}$. Applying the root test gives
    \[
        \beta = \limsup |a_n|^\frac{1}{n} = \limsup \qty|\frac{1}{n^2 2^n}|^\frac{1}{n} = \limsup \frac{1}{2 \sqrt[n]{n^2}} = \frac{1}{2}
    .\]
    Therefore $R = \frac{1}{\beta} = 2$ meaning $f$ is continuous on $(-2, 2)$. Note that
    \begin{alignat*}{2}
        f(2) &= \sum_{n=1}^\infty \frac{2^n}{n^2 2^n} = \sum_{n=1}^\infty \frac{1}{n^2} &&\implies \text{Converges} \\
        f(-2) &= \sum_{n=1}^\infty \frac{(-2)^n}{n^2 2^n} = \sum_{n=1}^\infty \frac{(-1)^n}{n^2} &&\implies \text{Converges}
    \end{alignat*}
    Therefore the interval of convergence is $[-2, 2]$. Important to note that there is no guarantee about continuity at the endpoints even though convergence is established.
\end{example}

\begin{example}
    Consider $g(x) = \displaystyle \sum_{n=0}^\infty 3^{-n} x^n$. Then
    \[
        \beta = \limsup |a_n|^\frac{1}{n} = \limsup \qty|3^{-n}|^\frac{1}{n} = \frac{1}{3}
    .\]
    Therefore $R = \frac{1}{\beta} = 3$ and hence $g$ is continuous on $(-3, 3)$. Note that
    \begin{alignat*}{2}
        g(3) &= \sum_{n=0}^\infty 3^{-n} 3^n = \sum_{n=0}^\infty 1 &&\implies \text{Diverges} \\
        g(-3) &= \sum_{n=0}^\infty 3^{-n} (-3)^n = \sum_{n=0}^\infty (-1)^n &&\implies \text{Diverges} \\
    \end{alignat*}
    Hence the interval of convergence is $(-3, 3)$.
\end{example}

\begin{lemma}
    If $\sum a_n x^n$ has a radius of convergence $R$, then $\sum \frac{a_n}{n+1} x^{n+1}$ has a radius of convergence $R$.
\end{lemma}

\begin{proof}
    Let $\beta = \frac{1}{R}$. The second series can be rewritten as
    \[
        \sum \frac{a_n}{n+1} x^{n+1} = x \sum \frac{a_n}{n+1} x^n = x \sum b_n x^n
    .\]
    Applying the root test to this new series gives
    \[
        \tilde{\beta} = \limsup |b_n|^\frac{1}{n} = \limsup \qty|\frac{a_n}{n+1}|^\frac{1}{n} = \frac{\limsup |a_n|^\frac{1}{n}}{\limsup |n+1|^\frac{1}{n}} = \frac{\beta}{1} = \beta
    .\]
    Therefore $\tilde{R} = \frac{1}{\tilde{\beta}} = \frac{1}{\beta} = R$.
\end{proof}

\begin{theorem}
    If $f(x) = \displaystyle \sum_{n=0}^\infty a_n x^n$ has radius of convergence $R$, then
    \[
        \int_0^x f(t) dt = \sum_{n=0}^\infty \frac{a_n}{n+1} x^{n+1}
    \]
    for all $|x| < R$.
\end{theorem}

\begin{proof}
    First note that
    \[
        \int_0^x f(t) \dd t = \int_0^x \sum_{k=0}^\infty a_k t^k \dd t = \int_0^x \lim_{n \to \infty} \sum_{k=0}^n a_k t^k \dd t
    .\]
    For $|x| < R$, the power series will uniformly converge so the limit can be swapped giving
    \begin{align*}
        \int_0^x \lim_{n \to \infty} \sum_{k=0}^n a_k t^k \dd t = 
        \lim_{n\to \infty} \int_0^x \sum_{k=0}^n a_k t^k \dd t &= 
        \sum_{k=0}^n \int_0^x a_k t^k \dd t \\
       &= \lim_{n\to \infty} \sum_{k=0}^\infty a_k \cdot \frac{x^{k+1}}{k+1} \\
       &= \sum_{k=0}^\infty \frac{a_k}{k+1} x^{k+1}
    \end{align*}
\end{proof}

\begin{lemma}
    If $\sum a_n x^n$ has a radius of converge $R$, then $\sum na_n x^{n-1}$ has a radius of convergence $R$.
\end{lemma}
\begin{proof}
    Let $b_n = (n+1) a_{n+1}$ and $\beta = \frac{1}{R}$. Then
    \begin{align*}
        \tilde{\beta} = \limsup |b_n|^\frac{1}{n} &= 
        \limsup \qty|(n+1)a_n+1|^\frac{1}{n}  \\
        &= \lim (n+1)^\frac{1}{n} \limsup |a_n+1|^\frac{1}{n} \\
        &= 1 \cdot \limsup \qty(|a_{n+1}|^\frac{1}{n})^\frac{n+1}{n} \\
        &= \limsup |a_{n+1}|^\frac{1}{n+1} \cdot \limsup \qty(|a_{n+1}|^\frac{1}{n+1})^\frac{1}{n} = \beta \cdot 1 = \beta
    \end{align*}
    Therefore their radius of convergence are the same.
\end{proof}

\begin{theorem}
    If $f(x) = \displaystyle \sum_{n=0}^\infty a_n x^n$ has radius of convergence $R$, the $f$ is differentiable on $(-R, R)$ and $f'(x) = \displaystyle \sum_{n=1}^\infty n a_n x^{n-1}$.
\end{theorem}

\begin{proof}
    Let $g(x) = \sum_{n=1}^\infty n a_n x^{n-1}$. Then
    \[
        G(x) = \int_0^x g(t) \dd t = \int_0^x \sum_{n=1}^\infty n a_n t^{n-1} \dd t = \sum_{n=1}^\infty n a_n \cdot \frac{x^n}{n} = \sum_{n=1}^\infty a_n x^n = f(x) - f(0)
    .\]
    Therefore $G'(x) = g(x) \implies f'(x) = g(x)$ for all $|x| < R$.
\end{proof}

\end{document}
